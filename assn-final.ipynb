{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from transformers import pipeline\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_reddit(client_id, client_secret):\n",
    "    return praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=\"stock_sentiment_analyzer/1.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sentiment_analyzer():\n",
    "    return pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit_content(reddit, company_ticker, subreddits=None, days=365):\n",
    "    subreddits = subreddits or ['stocks', 'investing', 'wallstreetbets', 'pennystocks', 'stockmarket']\n",
    "\n",
    "    scraped_content = []\n",
    "\n",
    "    for subreddit_name in subreddits:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            posts = subreddit.search(\n",
    "                query=company_ticker,\n",
    "                sort='new',\n",
    "                time_filter='all',\n",
    "                limit=250\n",
    "            )\n",
    "\n",
    "            for post in posts:\n",
    "                post_age = datetime.utcnow() - datetime.fromtimestamp(post.created_utc)\n",
    "                if post_age.days <= days:\n",
    "                    content = {\n",
    "                        'title': post.title,\n",
    "                        'text': post.selftext or '',\n",
    "                        'score': post.score,\n",
    "                        'comments_count': post.num_comments,\n",
    "                        'created_at': datetime.fromtimestamp(post.created_utc)\n",
    "                    }\n",
    "                    scraped_content.append(content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {subreddit_name}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(scraped_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(sentiment_analyzer, data):\n",
    "    sentiment_results = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        text = f\"{row['title']} {row['text']}\"\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        try:\n",
    "            sentiment = sentiment_analyzer(text[:512])[0]\n",
    "            sentiment_results.append({\n",
    "                'created_at': row['created_at'],\n",
    "                'sentiment_label': sentiment['label'],\n",
    "                'sentiment_score': sentiment['score'] if sentiment['label'] == 'POSITIVE' else -sentiment['score'],\n",
    "                'post_score': row['score'],\n",
    "                'comments_count': row['comments_count']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Sentiment analysis error: {e}\")\n",
    "\n",
    "    return pd.DataFrame(sentiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(stock_data, sentiment_data, lookback=30):\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "    sentiment_agg = sentiment_data.groupby(\n",
    "        pd.to_datetime(sentiment_data['created_at']).dt.date\n",
    "    ).agg({\n",
    "        'sentiment_score': 'mean',\n",
    "        'post_score': 'sum',\n",
    "        'comments_count': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    for col in ['sentiment_score', 'post_score', 'comments_count']:\n",
    "        if col not in stock_data.columns:\n",
    "            stock_data[col] = 0\n",
    "\n",
    "    merged_data = stock_data.copy()\n",
    "    merged_data['sentiment_score'] = merged_data['Date'].map(\n",
    "        dict(zip(sentiment_agg['created_at'], sentiment_agg['sentiment_score']))\n",
    "    ).fillna(0)\n",
    "    merged_data['post_score'] = merged_data['Date'].map(\n",
    "        dict(zip(sentiment_agg['created_at'], sentiment_agg['post_score']))\n",
    "    ).fillna(0)\n",
    "    merged_data['comments_count'] = merged_data['Date'].map(\n",
    "        dict(zip(sentiment_agg['created_at'], sentiment_agg['comments_count']))\n",
    "    ).fillna(0)\n",
    "\n",
    "    features = ['Close', 'Volume', 'sentiment_score', 'post_score', 'comments_count']\n",
    "    data = merged_data[features].values\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - lookback):\n",
    "        X.append(scaled_data[i:i + lookback, :])\n",
    "        y.append(scaled_data[i + lookback, 0])\n",
    "\n",
    "    return np.array(X), np.array(y), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', input_shape=input_shape, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, X_test, y_train, y_test):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=250,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, scaler):\n",
    "    predictions = model.predict(X_test).flatten()\n",
    "\n",
    "    y_test_full = np.zeros((y_test.shape[0], scaler.scale_.shape[0]))\n",
    "    y_test_full[:, 0] = y_test\n",
    "\n",
    "    predictions_full = np.zeros((predictions.shape[0], scaler.scale_.shape[0]))\n",
    "    predictions_full[:, 0] = predictions\n",
    "\n",
    "    y_test_original = scaler.inverse_transform(y_test_full)[:, 0]\n",
    "    predictions_original = scaler.inverse_transform(predictions_full)[:, 0]\n",
    "\n",
    "    mse = mean_squared_error(y_test_original, predictions_original)\n",
    "    mae = mean_absolute_error(y_test_original, predictions_original)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "    return predictions_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, stock_data, scaler, lookback=30):\n",
    "    features = ['Close', 'Volume', 'sentiment_score', 'post_score', 'comments_count']\n",
    "    data = stock_data[features].values\n",
    "\n",
    "    scaled_data = scaler.transform(data)\n",
    "\n",
    "    last_sequence = scaled_data[-lookback:]\n",
    "    last_sequence = last_sequence.reshape(1, lookback, -1)\n",
    "\n",
    "    predicted_scaled = model.predict(last_sequence).flatten()\n",
    "\n",
    "    prediction_full = np.zeros((1, scaler.scale_.shape[0]))\n",
    "    prediction_full[0, 0] = predicted_scaled\n",
    "    predicted_price = scaler.inverse_transform(prediction_full)[0, 0]\n",
    "\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit data downloaded.\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Mean Squared Error: 4.859606178610601\n",
      "Mean Absolute Error: 1.5641132972114196\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "Predicted Future Price: $46.56\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "#     TICKER = 'JANX'\n",
    "#     CLIENT_ID = 'TYrklKt5KWR9ljrcZqyJPQ'\n",
    "#     CLIENT_SECRET = '_RbOwvRafkLDwQe6KkWTqM_LHLPPvA'\n",
    "\n",
    "#     reddit = initialize_reddit(CLIENT_ID, CLIENT_SECRET)\n",
    "#     sentiment_analyzer = initialize_sentiment_analyzer()\n",
    "\n",
    "#     stock_data = yf.download(TICKER, start=datetime.now() - timedelta(days=1825))\n",
    "#     stock_data.reset_index(inplace=True)\n",
    "#     stock_data['Date'] = stock_data['Date'].dt.date\n",
    "\n",
    "#     reddit_data = scrape_reddit_content(reddit, TICKER)\n",
    "#     print(\"Reddit data downloaded.\")\n",
    "#     sentiment_data = analyze_sentiment(sentiment_analyzer, reddit_data)\n",
    "#     sentiment_data['created_at'] = sentiment_data['created_at'].dt.date\n",
    "\n",
    "#     X, y, scaler = prepare_dataset(stock_data, sentiment_data)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     model = create_model(X_train.shape[1:])\n",
    "#     train_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "#     predictions = evaluate_model(model, X_test, y_test, scaler)\n",
    "\n",
    "#     future_price = predict_future(model, stock_data, scaler)\n",
    "#     print(f\"Predicted Future Price: ${future_price:.2f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit data downloaded.\n"
     ]
    }
   ],
   "source": [
    "TICKER = 'AAPL'         #Change this to the desired stock\n",
    "CLIENT_ID = ''          #Change the API keys fo reddit\n",
    "CLIENT_SECRET = ''\n",
    "\n",
    "reddit = initialize_reddit(CLIENT_ID, CLIENT_SECRET)\n",
    "sentiment_analyzer = initialize_sentiment_analyzer()\n",
    "\n",
    "stock_data = yf.download(TICKER, start=datetime.now() - timedelta(days=1825))\n",
    "stock_data.reset_index(inplace=True)\n",
    "stock_data['Date'] = stock_data['Date'].dt.date\n",
    "\n",
    "reddit_data = scrape_reddit_content(reddit, TICKER)\n",
    "print(\"Reddit data downloaded.\")\n",
    "sentiment_data = analyze_sentiment(sentiment_analyzer, reddit_data)\n",
    "sentiment_data['created_at'] = sentiment_data['created_at'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>post_score</th>\n",
       "      <th>comments_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.995668</td>\n",
       "      <td>8</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-22</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.995668</td>\n",
       "      <td>13</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.995668</td>\n",
       "      <td>22</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-08</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.995668</td>\n",
       "      <td>16</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.975987</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.748695</td>\n",
       "      <td>157</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>2023-12-14</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.947190</td>\n",
       "      <td>45</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>2023-12-14</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.994136</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.985799</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.994327</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     created_at sentiment_label  sentiment_score  post_score  comments_count\n",
       "0    2024-11-29        NEGATIVE        -0.995668           8             174\n",
       "1    2024-11-22        NEGATIVE        -0.995668          13             255\n",
       "2    2024-11-15        NEGATIVE        -0.995668          22             481\n",
       "3    2024-11-08        NEGATIVE        -0.995668          16             413\n",
       "4    2024-11-02        NEGATIVE        -0.975987          28               9\n",
       "..          ...             ...              ...         ...             ...\n",
       "622  2023-12-16        POSITIVE         0.748695         157             213\n",
       "623  2023-12-14        NEGATIVE        -0.947190          45              16\n",
       "624  2023-12-14        POSITIVE         0.994136           0              16\n",
       "625  2023-12-08        POSITIVE         0.985799           1               3\n",
       "626  2023-12-07        NEGATIVE        -0.994327           8              13\n",
       "\n",
       "[627 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "Mean Squared Error: 43.918962999743044\n",
      "Mean Absolute Error: 5.347785542429231\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Predicted Future Price: $220.40\n"
     ]
    }
   ],
   "source": [
    "X, y, scaler = prepare_dataset(stock_data, sentiment_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = create_model(X_train.shape[1:])\n",
    "train_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "predictions = evaluate_model(model, X_test, y_test, scaler)\n",
    "\n",
    "future_price = predict_future(model, stock_data, scaler)\n",
    "print(f\"Predicted Future Price: ${future_price:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
